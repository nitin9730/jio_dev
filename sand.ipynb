{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea712127",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Jun 24 11:09:17 2024\n",
    "@author: suraj2.desai\n",
    "\"\"\"\n",
    "from sshtunnel import SSHTunnelForwarder\n",
    "import mysql.connector\n",
    "import psycopg2\n",
    "from datetime import timedelta\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import pymysql\n",
    "# Define your SSH and MySQL details\n",
    "ssh_host = '10.134.72.5'\n",
    "ssh_port = 22\n",
    "ssh_user = 'fynd'\n",
    "ssh_private_key =  r'/Users/Nitin14.Patil/Downloads/ssh_key.fynd'\n",
    " \n",
    "\n",
    "mysql_host = '172.17.0.38'  # This should be 127.0.0.1 because we are using SSH tunneling\n",
    "mysql_port = 3306\n",
    "mysql_user = 'fynd_avis_read'\n",
    "mysql_password = 'fynd_avis_read!2022'\n",
    "mysql_database = 'avis'\n",
    "fromDate = datetime(2024, 4, 1, 0, 0, 0)\n",
    "toDate = datetime(2024, 9, 21, 15, 56, 0) \n",
    "from_timestamp = int(fromDate.timestamp())\n",
    "to_timestamp = int(toDate.timestamp())\n",
    "\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT s.fynd_order_id,\n",
    "s.id as \"shipment_id\",\n",
    "FROM_UNIXTIME(o.created_ts) as \"order_date\",\n",
    "pm.mode,\n",
    "ss.status,\n",
    "b.seller_identifier as \"article_code\",\n",
    "b.id as \"bag_id\",\n",
    "i.name as \"item_name\",\n",
    "i.brand,\n",
    "i.l1_category ,\n",
    "i.l2_category ,\n",
    "i.l3_category ,\n",
    "b.item_id,\n",
    "b.gstin_code,\n",
    "b.quantity ,\n",
    "b.journey_type,\n",
    "s2.code as \"store_code\",\n",
    "b.store_id ,\n",
    "s.store_invoice_id ,\n",
    "s.credit_note_id,\n",
    "payment_mode_id ,\n",
    "source,ordering_channel,\n",
    "s.delivery_awb_number,\n",
    "s.delivery_address_json,\n",
    "b.prices,\n",
    "b.article_json,\n",
    "b.meta,\n",
    "b.applied_promos,\n",
    "s.meta as \"shipment_meta\"\n",
    "FROM avis.`order` o\n",
    "LEFT JOIN shipment s ON o.fynd_order_id = s.fynd_order_id\n",
    "LEFT JOIN (SELECT shipment_id, status, created_at\n",
    "FROM (SELECT shipment_id,status,created_at, ROW_NUMBER() OVER (PARTITION BY shipment_id ORDER BY created_ts DESC ,updated_ts DESC, id DESC) AS row_num FROM shipment_status) AS subquery\n",
    "WHERE row_num = 1) ss ON s.id = ss.shipment_id\n",
    "LEFT JOIN bag b on s.id = b.shipment_id\n",
    "LEFT JOIN item i on b.item_id = i.id\n",
    "LEFT JOIN payment_mode pm on o.payment_mode_id = pm.id\n",
    "LEFT JOIN store s2 on b.store_id = s2.id\n",
    "WHERE o.fynd_order_id like 'SS%' \n",
    " \n",
    "and\n",
    "o.created_ts >= {from_timestamp} \n",
    "AND o.created_ts <= {to_timestamp}\n",
    "\"\"\".format(from_timestamp=from_timestamp, to_timestamp=to_timestamp)\n",
    "with SSHTunnelForwarder(\n",
    "    (ssh_host, ssh_port),\n",
    "    ssh_username=ssh_user,\n",
    "    ssh_pkey=ssh_private_key,\n",
    "    remote_bind_address=(mysql_host, mysql_port)\n",
    ") as tunnel:\n",
    "    # Connect to the MySQL database using pymysql\n",
    "    connection = pymysql.connect(\n",
    "        user=mysql_user,\n",
    "        password=mysql_password,\n",
    "        host='127.0.0.1',\n",
    "        port=tunnel.local_bind_port,\n",
    "        database=mysql_database\n",
    "    )\n",
    "    # Create a cursor object\n",
    "    cursor = connection.cursor()\n",
    "    # Disable ONLY_FULL_GROUP_BY for the current session\n",
    "    cursor.execute(\"SET sql_mode=(SELECT REPLACE(@@sql_mode,'ONLY_FULL_GROUP_BY',''));\")\n",
    "    cursor.execute(query)\n",
    "    result = cursor.fetchall()\n",
    "    connection.close()\n",
    "    # Convert the result to a Pandas DataFrame\n",
    "    columns = [column[0] for column in cursor.description]\n",
    "    df = pd.DataFrame(result, columns=columns)\n",
    " \n",
    "    df1 = df[['fynd_order_id','shipment_id','article_code','bag_id','delivery_address_json','prices','article_json','meta','shipment_meta']]\n",
    "    df2 = df1.values.tolist()\n",
    "    df3 = df2[1][6]\n",
    "    result = pd.DataFrame()\n",
    "    result1 = pd.DataFrame()\n",
    "    result2 = pd.DataFrame()\n",
    "    result3 = pd.DataFrame()\n",
    "    result4 = pd.DataFrame()\n",
    "    df5 = pd.DataFrame()\n",
    "    df8 = pd.DataFrame()\n",
    "    for n in range(len(df2)):\n",
    "        df3 = df2[n][6]\n",
    "        if df3 is not None:\n",
    "            df4 = json.loads(df3)\n",
    "            df5 = pd.json_normalize(df4)[['size', 'price_marked']]\n",
    "            df5['fynd_order_id'] = df2[n][0]\n",
    "            df5['shipment_id'] = df2[n][1]\n",
    "            df5['article_code'] = df2[n][2]\n",
    "            df5['bag_id'] = df2[n][3]\n",
    "            df5['identifier'] = [df4['identifier']]\n",
    "            df5 = df5[['fynd_order_id','shipment_id','article_code','bag_id','size','price_marked','identifier']]\n",
    "        if df3 is None:\n",
    "            df5['fynd_order_id'] = df2[n][0]\n",
    "            df5['shipment_id'] = df2[n][1]\n",
    "            df5['article_code'] = df2[n][2]\n",
    "            df5['bag_id'] = df2[n][3]\n",
    "            df5['size'] = ''\n",
    "            df5['price_marked'] = ''\n",
    "            df5['identifier'] = ''\n",
    "            df5 = df5[['fynd_order_id','shipment_id','article_code','bag_id','size','price_marked','identifier']]\n",
    "        result = pd.concat([result, df5], axis=0, ignore_index=True)\n",
    "        #df6 = pd.json_normalize(df4, meta=['size', 'price_marked', 'identifier.ean','identifier.sku_code'])\n",
    " \n",
    " \n",
    "    for n in range(len(df2)):\n",
    "        df3 = df2[n][4]\n",
    "        df4 = json.loads(df3)\n",
    "        df5 = pd.json_normalize(df4)\n",
    "        df5['fynd_order_id'] = df2[n][0]\n",
    "        df5['shipment_id'] = df2[n][1]\n",
    "        df5['article_code'] = df2[n][2]\n",
    "        df5['bag_id'] = df2[n][3]\n",
    "        result1 = pd.concat([result1, df5], axis=0, ignore_index=True)\n",
    " \n",
    "    for n in range(len(df2)):\n",
    "        df3 = df2[n][5]\n",
    "        if df3 is not None:\n",
    "            df4 = json.loads(df3)\n",
    "            df5 = pd.json_normalize(df4['amount_paid']['amount'])\n",
    "            df6 = pd.json_normalize(df4['price_marked']['amount'])\n",
    "            df7 = pd.json_normalize(df4['discount']['amount'])\n",
    "            df14 =pd.json_normalize(df4['coupon_effective_discount']['amount'])\n",
    "            df15 =pd.json_normalize(df4['promotion_effective_discount']['amount'])                      \n",
    "            df8 = pd.concat([df5, df6, df7,df14,df15], axis=1, ignore_index=True)\n",
    "            df8 = df8[[0,2,4,6,8]]\n",
    "            df8 = df8.rename(columns={0: 'amount_paid', 2: 'price_marked', 4: 'discount',6:'coupon_applied',8:'promo_applied'})\n",
    "            df8['fynd_order_id'] = df2[n][0]\n",
    "            df8['shipment_id'] = df2[n][1]\n",
    "            df8['article_code'] = df2[n][2]\n",
    "            df8['bag_id'] = df2[n][3]\n",
    "            df8 = df8[['fynd_order_id','shipment_id','article_code','bag_id','amount_paid','price_marked','discount','coupon_applied','promo_applied']]\n",
    "        if df3 is None:\n",
    "            df8['fynd_order_id'] = df2[n][0]\n",
    "            df8['shipment_id'] = df2[n][1]\n",
    "            df8['article_code'] = df2[n][2]\n",
    "            df8['bag_id'] = df2[n][3]\n",
    "            df8['amount_paid'] = ''\n",
    "            df8['price_marked'] = ''\n",
    "            df8['discount'] = ''\n",
    "            df8['coupon_applied'] = ''\n",
    "            df8['promo_applied'] = ''\n",
    "            df8 = df8[['fynd_order_id','shipment_id','article_code','bag_id','amount_paid','price_marked','discount','coupon_applied','promo_applied']]\n",
    "        result2 = pd.concat([result2, df8], axis=0, ignore_index=True)\n",
    "    for n in range(len(df2)):\n",
    "        df3 = df2[n][7]\n",
    "        if df3 is not None:\n",
    "            df4 = json.loads(df3)\n",
    "            if 'serial_numbers' in df4:\n",
    "                df5['serial_numbers'] = [df4['serial_numbers']]\n",
    "                df5['fynd_order_id'] = df2[n][0]\n",
    "                df5['shipment_id'] = df2[n][1]\n",
    "                df5['article_code'] = df2[n][2]\n",
    "                df5['bag_id'] = df2[n][3]\n",
    "            if 'serial_numbers' not in df4:\n",
    "                df5['serial_numbers'] = ''\n",
    "                df5['fynd_order_id'] = df2[n][0]\n",
    "                df5['shipment_id'] = df2[n][1]\n",
    "                df5['article_code'] = df2[n][2]\n",
    "                df5['bag_id'] = df2[n][3]\n",
    "        if df3 is None:\n",
    "            df5['serial_numbers'] = ''\n",
    "            df5['fynd_order_id'] = df2[n][0]\n",
    "            df5['shipment_id'] = df2[n][1]\n",
    "            df5['article_code'] = df2[n][2]\n",
    "            df5['bag_id'] = df2[n][3]\n",
    "        result3 = pd.concat([result3, df5], axis=0, ignore_index=True)\n",
    "    for n in range(len(df2)):\n",
    "        df3 = df2[n][8]\n",
    "        df4 = json.loads(df3)\n",
    "        if any(key.startswith('article_level_discount') for key in df4):\n",
    "                for key in df4:\n",
    "                    if key.startswith('article_level_discount'):\n",
    "                        df5 = pd.json_normalize(df4[key])\n",
    "                        df5['fynd_order_id'] = df2[n][0]\n",
    "                        df5['bag_id'] = df2[n][3]\n",
    "        else:\n",
    "           df5['bagId'] = ''\n",
    "           df5['lineNo'] = ''\n",
    "           df5['articleId'] = ''\n",
    "           df5['articlePrice'] = ''\n",
    "           df5['discountAmount'] = ''\n",
    "           df5['fynd_order_id'] = df2[n][0]\n",
    "           df5['bag_id'] = df2[n][3]\n",
    "        result4 = pd.concat([result4, df5], axis=0, ignore_index=True)\n",
    " \n",
    " \n",
    "    final_dataframe = pd.merge(df, result, on = ['fynd_order_id','shipment_id','article_code','bag_id'], how = 'inner')\n",
    "    final_dataframe1 = pd.merge(final_dataframe, result1, on = ['fynd_order_id','shipment_id','article_code','bag_id'], how = 'inner')\n",
    "    final_dataframe2 = pd.merge(final_dataframe1, result2, on = ['fynd_order_id','shipment_id','article_code','bag_id'], how = 'inner')\n",
    "    final_dataframe3 = pd.merge(final_dataframe2, result3, on = ['fynd_order_id','shipment_id','article_code','bag_id'], how = 'inner')\n",
    "    final_dataframe4 = pd.merge(final_dataframe3, result4, on = ['fynd_order_id','bag_id'], how = 'left')\n",
    "    \n",
    " \n",
    "final_dataframe4['Final_status']  = np.where(final_dataframe4['credit_note_id'].isna(), final_dataframe4['status'], 'handed_over_to_customer') \n",
    "\n",
    "\n",
    "# Sample dataframe\n",
    "\n",
    "\n",
    "# Create column 'c' where column 'b' contains 'xx'\n",
    "final_dataframe4['SMKITTI'] = ''\n",
    "final_dataframe4.loc[final_dataframe4['applied_promos'].str.contains('SMKITTI',na=False), 'SMKITTI'] = 'Yes'\n",
    "\n",
    "print(df)\n",
    "\n",
    "# and\n",
    "# ss.status not in ('pending','payment_failed')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "date = time.strftime(\"%Y%m%d%H%M%S\")\n",
    "final_dataframe4 = final_dataframe4.replace('\\n', '-', regex=True)\n",
    "final_dataframe4.to_csv(rf'\\Users\\Nitin14.Patil\\Downloads\\order_discount{date}.txt', sep = '|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443ea415",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import time\n",
    "import datetime\n",
    "from dateutil import parser\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from sshtunnel import SSHTunnelForwarder\n",
    "import psycopg2\n",
    "import paramiko\n",
    "# SSH server information\n",
    "ssh_host = '10.134.72.5'\n",
    "ssh_port = 22\n",
    "ssh_username = 'fynd'\n",
    "private_key_path = '/Users/Nitin14.Patil/Downloads/ssh_key.fynd'\n",
    "# Database server information\n",
    "db_host = '172.17.0.16'\n",
    "db_port = 5432\n",
    "db_username = 'fynd_hogwarts_read'\n",
    "db_password = 'fynd_hogwarts_read!2022'\n",
    " \n",
    "\"\"\"\n",
    "sql = '''select\n",
    "t.primary_identifier,\n",
    "t.state,\n",
    " \n",
    " \n",
    "t.status,\n",
    " \n",
    " \n",
    "t.data,\n",
    " \n",
    " \n",
    "te.payload\n",
    " \n",
    " \n",
    "from task t\n",
    " \n",
    " \n",
    "join\n",
    " \n",
    " \n",
    "task_events te\n",
    " \n",
    " \n",
    "on t.uid = te.task_id\n",
    " \n",
    " \n",
    "where\n",
    " \n",
    "t.state='placed' and\n",
    " \n",
    "t.status not in ('stopped', 'failed') and\n",
    " \n",
    "identifier_type = 'fynd_order' and\n",
    " \n",
    "te.payload <> 'None' and\n",
    " \n",
    "t.created_on >= '{compare_date}' '''.format(compare_date = datetime.datetime.now() - datetime.timedelta(minutes= 10))\n",
    " \n",
    "\"\"\"\n",
    "\"\"\"\n",
    "sql = '''select\n",
    " \n",
    "t.primary_identifier,\n",
    " \n",
    " \n",
    "t.state,\n",
    " \n",
    " \n",
    "t.status,\n",
    " \n",
    " \n",
    "t.data,\n",
    " \n",
    " \n",
    "te.payload\n",
    " \n",
    " \n",
    "from task t\n",
    " \n",
    " \n",
    "join\n",
    " \n",
    " \n",
    "task_events te\n",
    " \n",
    " \n",
    "on t.uid = te.task_id\n",
    " \n",
    " \n",
    "where\n",
    " \n",
    "t.state='placed' and\n",
    " \n",
    "t.status not in ('stopped', 'failed') and\n",
    " \n",
    "t.identifier_type = 'fynd_order' and\n",
    " \n",
    "t.primary_identifier in (\n",
    "\n",
    "'FY6568B8A80E1D112AD7',\n",
    "'FY656B303C0E2FC06551',\n",
    "'FY656F36720EB3099EA5',\n",
    "'FY6572BE090EA692F0BE',\n",
    "'FY6578640B0E70227AE9',\n",
    "'FY657AB1CB0ECAC47D9F',\n",
    "'FY6568A93A0EBE573EC8',\n",
    "'FY656C88140EC03B0AE6',\n",
    "'FY656DB6A10EC71E0A3D',\n",
    "'FY65770E5A0ECA1990DD',\n",
    "'FY657348D60E9B5168B4',\n",
    "'FY6583E5110E4E0C2C8C',\n",
    "'FY65689DDA0E6CC3505B',\n",
    "'FY657441830E3CB1F028',\n",
    "'FY6579A5A40ED2C94D33',\n",
    "'FY6579BD6F0E7CB90A8B',\n",
    "'FY656B4A790EF348557B',\n",
    "'FY656F0A950EB9FF651C',\n",
    "'FY65E079140EF9162D52'\n",
    "                         \n",
    "                         \n",
    ") and\n",
    " \n",
    "te.payload <> 'None'\n",
    " \n",
    "'''\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "sql = '''select \n",
    " \n",
    " \n",
    "t.primary_identifier,\n",
    " \n",
    " \n",
    "t.state,\n",
    " \n",
    " \n",
    "t.status,\n",
    " \n",
    " \n",
    "t.data,\n",
    " \n",
    " \n",
    "te.payload\n",
    " \n",
    " \n",
    "from task t\n",
    " \n",
    " \n",
    "join\n",
    " \n",
    " \n",
    "task_events te\n",
    " \n",
    " \n",
    "on t.uid = te.task_id\n",
    " \n",
    " \n",
    "where\n",
    " \n",
    "t.state='placed' and\n",
    " \n",
    "t.status not in ('stopped', 'failed') and\n",
    " \n",
    "identifier_type = 'fynd_order' and\n",
    " \n",
    "te.payload <> 'None' and\n",
    " \n",
    "t.created_on >= '{fromDate}' and\n",
    " \n",
    "t.created_on <= '{toDate}' '''.format(fromDate = datetime.datetime(2024, 9, 15, 0, 0, 0, 0), toDate = datetime.datetime(2024, 9, 27, 0, 0, 0, 0))\n",
    "\n",
    "\n",
    "ssh_key = paramiko.RSAKey.from_private_key_file(private_key_path)\n",
    " \n",
    " \n",
    " \n",
    "with SSHTunnelForwarder(\n",
    "    (ssh_host, ssh_port),\n",
    "    ssh_username=ssh_username,\n",
    "    ssh_pkey=ssh_key,\n",
    "    remote_bind_address=(db_host, db_port)\n",
    ") as tunnel:\n",
    "\n",
    "    conn = psycopg2.connect(\n",
    "        host='localhost',\n",
    "        port=tunnel.local_bind_port,\n",
    "        user=db_username,\n",
    "        password=db_password,\n",
    "        database='hogwarts'\n",
    "    )\n",
    " \n",
    " \n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"select version()\")    \n",
    "    data = cursor.fetchone()\n",
    "    print(\"Connection established to: \",data)\n",
    " \n",
    " \n",
    "    cursor.execute(sql)\n",
    "    results = cursor.fetchall()\n",
    "    print(sql)\n",
    " \n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "data=results\n",
    "\n",
    "\n",
    "list_json=[]\n",
    "vas_json=[]\n",
    "for t in results:\n",
    "    list_json.append(list(t))\n",
    "for n in range(len(list_json)):   \n",
    "    a=list_json[n][4]\n",
    "    a=eval(a)\n",
    "    if 'products' in a and any(product.get('ITEM_VAL') == 'VAS' for product in a['products']):\n",
    "    \n",
    "        vas_json.append(a)\n",
    "\n",
    "meta_d=pd.DataFrame()\n",
    "for n1 in range(len(vas_json)):\n",
    "    keys_to_extract = ['temp_order_id', 'customer_name','contact_number','billing_address','email_id','order_created_date']\n",
    "    extracted_values = [[d[key] for key in keys_to_extract] for d in vas_json]\n",
    "    \n",
    "    df = pd.DataFrame(extracted_values, columns=keys_to_extract)\n",
    "\n",
    "    ajc=pd.json_normalize(vas_json[n1])\n",
    "    Article_aj=pd.json_normalize(ajc['products'])\n",
    "    meta=Article_aj[0].to_dict()\n",
    "    meta_df=pd.json_normalize(meta)\n",
    "    \n",
    "    meta_df1=meta_df[['0.REF_NO','0.article_id']]\n",
    "    \n",
    "    meta_d=pd.concat([meta_d,meta_df1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "meta_d=meta_d.reset_index()\n",
    "meta_d=meta_d.drop(columns=['index'])\n",
    "\n",
    "\n",
    "try:\n",
    "\n",
    "    for n2 in range (len(df['billing_address'])):\n",
    "    \n",
    "    \n",
    "        my_dict=df['billing_address'][n2]\n",
    "    \n",
    "        keys_to_get = ['flat_no', 'building_name','street','sector','city','state','pin']\n",
    "    \n",
    "        selected_values = {key: my_dict[key] for key in keys_to_get if key in my_dict}\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        values = [str(value) for value in selected_values.values()]\n",
    "    \n",
    "        comma_separated_values = ' '.join(values)\n",
    "    \n",
    "    \n",
    "    \n",
    "        df['billing_address'][n2]=comma_separated_values\n",
    "        final_df=pd.concat([df,meta_d],axis=1,ignore_index=True)\n",
    "        column_names = ['temp_order_id', 'customer_name','contact_number','billing_address','email_id','order_created_date','imei_no','article_code']\n",
    "        final_df.columns=(column_names)\n",
    "\n",
    "        print(final_df)\n",
    "        final_df.to_csv('VAS_data6.csv',index=False)\n",
    "        \n",
    "except:\n",
    "    print('Messege:- There is no VAS entry Yesterday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24190017",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eec4fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Folder path containing the files\n",
    "folder_path = '/Users/nitin14.patil/Library/CloudStorage/OneDrive-RelianceCorporateITParkLimited/Documents/python_work/PNL Finance/vas/'\n",
    "\n",
    "# Get a list of all CSV files in the folder\n",
    "files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Loop through all the files and read them\n",
    "for file in files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    df = pd.read_csv(file_path)  # Adjust if using other formats like pd.read_excel()\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate all the DataFrames along axis=0\n",
    "merged_df = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(merged_df)\n",
    "\n",
    "merged_df.to_csv('vas data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fa5894",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime \n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "\n",
    "# Assuming you have the list of not_present_identifiers\n",
    "#not_present_identifiers = ['RD64A802990133A838FF', 'FYjewjwkjewfwe']\n",
    "\n",
    "\n",
    "\n",
    "# MongoDB connection\n",
    "client = MongoClient('mongodb://fynd_cofo_readwrite:readWrite_cofo!2023@10.134.32.72:27017,10.134.32.70:27017,10.134.32.71:27017/cofo?replicaSet=re_extensions_set&readPreference=secondaryPreferred')\n",
    "\n",
    "\n",
    "\n",
    "# Access the 'extensions' database\n",
    "db = client.cofo\n",
    "\n",
    "\n",
    "\n",
    "# Access the 'raven_order_proxy' collection\n",
    "collection = db.cofo_store_mapping\n",
    "\n",
    "\n",
    "\n",
    "date= time.strftime(\"%Y%m%d%H%M%S\")\n",
    "yesterday = datetime.now() - timedelta(days=1)\n",
    "yesterday_start = datetime(yesterday.year, yesterday.month, yesterday.day, 0, 0, 0, 0)\n",
    "yesterday_end = datetime(yesterday.year, yesterday.month, yesterday.day, 23, 59, 59, 999999)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#https://fyndsecure.perimeter81.com/sign-in?redirectUrl=perimeter81://perimeter81.com/macos/callback\n",
    "cursor= collection.find()\n",
    "list_curs = list(cursor)\n",
    "print(len(list_curs))\n",
    "df1 = pd.DataFrame(list_curs)\n",
    "# df1.to_csv(f'/Users/Nitin14.Patil/Documents/MONGODB/werks{date}.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import psycopg2\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from datetime import datetime, timedelta, timezone  \n",
    "import pandas as pd\n",
    "\n",
    "\n",
    " \n",
    "from sshtunnel import SSHTunnelForwarder\n",
    "\n",
    "import paramiko \n",
    "# SSH server information\n",
    "\n",
    "ssh_host = '10.134.72.5'\n",
    "\n",
    "ssh_port = 22\n",
    "\n",
    "ssh_username = 'fynd'\n",
    "\n",
    "private_key_path = '/Users/nitin14.patil/Downloads/ssh_key.fynd'\n",
    " \n",
    "# Database server information\n",
    "\n",
    "db_host = '172.17.0.80'\n",
    "\n",
    "db_port = 5432\n",
    "\n",
    "db_username = 'fynd_dms_orders_readwrite'\n",
    "\n",
    "db_password = 'readWrite_dms_orders!2024'\n",
    "last_two_hours_end_time = datetime.now(timezone.utc)\n",
    "last_two_hours_start_time = last_two_hours_end_time.replace(minute=0, second=0, microsecond=0) - timedelta(days=80)\n",
    " \n",
    "formatted_start_time = last_two_hours_start_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "formatted_end_time = last_two_hours_end_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    " \n",
    "date1 = time.strftime(\"%Y%m%d%H%M%S\")\n",
    "\"\"\"\n",
    " \n",
    "sql = '''select\n",
    " \n",
    " \n",
    "t.primary_identifier,\n",
    " \n",
    " \n",
    "t.state,\n",
    " \n",
    " \n",
    "t.status,\n",
    " \n",
    " \n",
    "t.data,\n",
    " \n",
    " \n",
    "te.payload\n",
    " \n",
    " \n",
    "from task t\n",
    " \n",
    " \n",
    "joinarticle_x\n",
    " \n",
    " \n",
    "task_events te\n",
    " \n",
    " \n",
    "on t.uid = te.task_id\n",
    " \n",
    " \n",
    "where\n",
    " \n",
    "t.state='placed' and\n",
    " \n",
    "t.status not in ('stopped', 'failed') and\n",
    " \n",
    "identifier_type = 'fynd_order' and\n",
    " \n",
    "te.payload <> 'None' and\n",
    " \n",
    "t.created_on >= '{fromDate}' and\n",
    " \n",
    "t.created_on <= '{toDate}' '''.format(fromDate = datetime.datetime(2024, 2, 13, 0, 0, 0, 0), toDate = datetime.datetime(2024, 2, 15, 0, 0, 0, 0))\n",
    " \n",
    "\"\"\"\n",
    " \n",
    "sql = f'''select o.channel_order_id,o.created_at  ,p.order_id ,p.quantity ,p.processed_quantity,p.sku,p.seq_id,o.sto_number ,o.prm_id,p.processed_unit_price ,srr.response_entity\n",
    "from orders o  \n",
    "left join products p on p.order_id = o.order_id\n",
    "left join sto_request_response srr on srr.channel_order_id =o.channel_order_id \n",
    "where\n",
    "o.created_at BETWEEN '{formatted_start_time}' AND '{formatted_end_time}' '''\n",
    "ssh_key = paramiko.RSAKey.from_private_key_file(private_key_path)\n",
    "\n",
    " \n",
    " \n",
    "with SSHTunnelForwarder(\n",
    "\n",
    "    (ssh_host, ssh_port),\n",
    "\n",
    "    ssh_username=ssh_username,\n",
    "\n",
    "    ssh_pkey=ssh_key,\n",
    "\n",
    "    remote_bind_address=(db_host, db_port)\n",
    "\n",
    ") as tunnel:\n",
    "\n",
    "    # Connect to the PostgreSQL database via the SSH tunnel\n",
    "\n",
    "    conn = psycopg2.connect(\n",
    "\n",
    "        host='localhost',\n",
    "\n",
    "        port=tunnel.local_bind_port,\n",
    "\n",
    "        user=db_username,\n",
    "\n",
    "        password=db_password,\n",
    "\n",
    "        database='dms_orders'\n",
    "\n",
    "    )\n",
    " \n",
    " \n",
    "    cur = conn.cursor()\n",
    " \n",
    " \n",
    "    cur.execute(sql)\n",
    "\n",
    "    results = cur.fetchall()\n",
    " \n",
    " \n",
    "    cur.close()\n",
    "\n",
    "    conn.close()\n",
    " \n",
    " \n",
    "print(sql)k\n",
    " \n",
    " \n",
    "len(results)\n",
    " \n",
    "print(len(results))\n",
    "\n",
    "\n",
    "\n",
    "print(results)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "df.columns = ['channel_order_id', 'created_at','order_id','quantity','processed_quantity','sku','seq_id','sto_number','prm_id','processed_unit_price','Error']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Sample JSON strings\n",
    "\n",
    "# Parse JSON strings and extract messages\n",
    "json_strings = df['Error'].values\n",
    "messages = []\n",
    "    \n",
    "for json_string in json_strings: \n",
    "    try: \n",
    "        json_data = json.loads(json_string)\n",
    "        return_data = json_data.get('t_RETURN')\n",
    "        if return_data:\n",
    "            \n",
    "            for item in return_data:\n",
    "                \n",
    "                message = item.get('message', '')\n",
    "                \n",
    "            messages.append(message)\n",
    "            print(len(messages))\n",
    "    except:\n",
    "        message='Null'\n",
    "        messages.append(message)\n",
    "        print(len(messages))\n",
    "\n",
    "\n",
    "# Create DataFrame\n",
    "dof = pd.DataFrame(messages, columns=['Message'])\n",
    "a=pd.DataFrame()\n",
    "df=pd.concat([df,dof],axis=1)\n",
    "# Display DataFrame\n",
    "df.drop(['Error'], axis=1,inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "result = pd.merge(df, df1, on='prm_id', how='left')\n",
    "\n",
    "print(result)\n",
    "\n",
    "print(df.columns)\n",
    "\n",
    "\n",
    "result12 = result[['channel_order_id', 'created_at', 'order_id', 'quantity',\n",
    "       'processed_quantity', 'sku', 'seq_id', 'sto_number', 'prm_id',\n",
    "       'processed_unit_price', 'Message', 'store_code']]\n",
    "\n",
    "\n",
    "result12.to_csv(f'/Users/nitin14.patil/Documents/MONGODB/Indent{date1}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9353e532",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b947e27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from datetime import datetime, timedelta, timezone  \n",
    "import pandas as pd\n",
    "\n",
    "\n",
    " \n",
    "from sshtunnel import SSHTunnelForwarder\n",
    "\n",
    "import paramiko \n",
    "# SSH server information\n",
    "\n",
    "ssh_host = '10.134.72.5'\n",
    "\n",
    "ssh_port = 22\n",
    "\n",
    "ssh_username = 'fynd'\n",
    "\n",
    "private_key_path = '/Users/nitin14.patil/Downloads/ssh_key.fynd'\n",
    " \n",
    "# Database server information\n",
    "\n",
    "db_host = '172.17.0.80'\n",
    "\n",
    "db_port = 5432\n",
    "\n",
    "db_username = 'fynd_dms_orders_readwrite'\n",
    "\n",
    "db_password = 'readWrite_dms_orders!2024'\n",
    "last_two_hours_end_time = datetime.now(timezone.utc)\n",
    "last_two_hours_start_time = last_two_hours_end_time.replace(minute=0, second=0, microsecond=0) - timedelta(days=80)\n",
    " \n",
    "formatted_start_time = last_two_hours_start_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "formatted_end_time = last_two_hours_end_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    " \n",
    "date1 = time.strftime(\"%Y%m%d%H%M%S\")\n",
    "\"\"\"\n",
    " \n",
    "sql = '''select\n",
    " \n",
    " \n",
    "t.primary_identifier,\n",
    " \n",
    " \n",
    "t.state,\n",
    " \n",
    " \n",
    "t.status,\n",
    " \n",
    " \n",
    "t.data,\n",
    " \n",
    " \n",
    "te.payload\n",
    " \n",
    " \n",
    "from task t\n",
    " \n",
    " \n",
    "joinarticle_x\n",
    " \n",
    " \n",
    "task_events te\n",
    " \n",
    " \n",
    "on t.uid = te.task_id\n",
    " \n",
    " \n",
    "where\n",
    " \n",
    "t.state='placed' and\n",
    " \n",
    "t.status not in ('stopped', 'failed') and\n",
    " \n",
    "identifier_type = 'fynd_order' and\n",
    " \n",
    "te.payload <> 'None' and\n",
    " \n",
    "t.created_on >= '{fromDate}' and\n",
    " \n",
    "t.created_on <= '{toDate}' '''.format(fromDate = datetime.datetime(2024, 2, 13, 0, 0, 0, 0), toDate = datetime.datetime(2024, 2, 15, 0, 0, 0, 0))\n",
    " \n",
    "\"\"\"\n",
    " \n",
    "sql = f'''select o.channel_order_id,o.created_at  ,p.order_id ,p.quantity ,p.processed_quantity,p.sku,p.seq_id,o.sto_number ,o.prm_id,p.processed_unit_price ,srr.response_entity\n",
    "from orders o  \n",
    "left join products p on p.order_id = o.order_id\n",
    "left join sto_request_response srr on srr.channel_order_id =o.channel_order_id \n",
    "where\n",
    "o.created_at BETWEEN '{formatted_start_time}' AND '{formatted_end_time}' '''\n",
    "ssh_key = paramiko.RSAKey.from_private_key_file(private_key_path)\n",
    "\n",
    " \n",
    " \n",
    "with SSHTunnelForwarder(\n",
    "\n",
    "    (ssh_host, ssh_port),\n",
    "\n",
    "    ssh_username=ssh_username,\n",
    "\n",
    "    ssh_pkey=ssh_key,\n",
    "\n",
    "    remote_bind_address=(db_host, db_port)\n",
    "\n",
    ") as tunnel:\n",
    "\n",
    "    # Connect to the PostgreSQL database via the SSH tunnel\n",
    "\n",
    "    conn = psycopg2.connect(\n",
    "\n",
    "        host='localhost',\n",
    "\n",
    "        port=tunnel.local_bind_port,\n",
    "\n",
    "        user=db_username,\n",
    "\n",
    "        password=db_password,\n",
    "\n",
    "        database='dms_orders'\n",
    "\n",
    "    )\n",
    " \n",
    " \n",
    "    cur = conn.cursor()\n",
    " \n",
    " \n",
    "    cur.execute(sql)\n",
    "\n",
    "    results = cur.fetchall()\n",
    " \n",
    " \n",
    "    cur.close()\n",
    "\n",
    "    conn.close()\n",
    " \n",
    " \n",
    "print(sql)\n",
    " \n",
    " \n",
    "len(results)\n",
    " \n",
    "print(len(results))\n",
    "\n",
    "\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee64c024",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "chrome_options = Options()\n",
    "# chrome_options.add_argument('--headless')\n",
    "chrome_options.add_argument('--incognito')\n",
    "\n",
    "browser = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()), options=chrome_options)\n",
    "\n",
    "\n",
    "\n",
    "# # Update this path to the location where you placed the downloaded chromedriver\n",
    "# chromedriver_path = \"/Users/nitin14.patil/Downloads/chromedriver\"\n",
    "\n",
    "# chrome_options = Options()\n",
    "# # chrome_options.add_argument('--headless')\n",
    "# # chrome_options.add_argument('--incognito')\n",
    "\n",
    "# chrome_service = ChromeService(executable_path=chromedriver_path)\n",
    "\n",
    "# browser = webdriver.Chrome(service=chrome_service, options=chrome_options) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "browser.get('https://in.prm.mi.com/#/enterprise_center/imei-report')\n",
    "\n",
    "browser.maximize_window()\n",
    "\n",
    "user_n=browser.find_elements(By.XPATH,\"//input[@name='account']\")\n",
    "user_p=browser.find_elements(By.XPATH,\"//input[@name='password']\")\n",
    "sleep(2)\n",
    "user_n[0].send_keys(\"8951981136\")\n",
    "user_p[0].send_keys(\"Ril@2024\")\n",
    "# user_n.send_keys(Keys.ENTER)\n",
    "sleep(2)\n",
    "# captcha_text\n",
    " #   image_element = browser.find_element(By.ID,\"Login1_imgCaptcha\")\n",
    "\n",
    "submit=browser.find_elements(By.XPATH,\"//button[@type='submit']\")\n",
    "\n",
    "submit[0].click()\n",
    "\n",
    "sleep(5)\n",
    "\n",
    "Enterprise_center=browser.find_elements(By.XPATH,\"//a[normalize-space()='Enterprise center']\")\n",
    "\n",
    "Enterprise_center[0].click()\n",
    "\n",
    "\n",
    "sleep(10)\n",
    "\n",
    "IMEI_SN=browser.find_elements(By.XPATH,\"//a[normalize-space()='IMEI/SN Report']\")\n",
    "\n",
    "IMEI_SN[0].click()\n",
    "\n",
    "\n",
    "sleep(5)\n",
    "\n",
    "print('calender expanction initiated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88d01fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
